\doxysection{include/\+Algorithms.hpp File Reference}
\hypertarget{_algorithms_8hpp}{}\label{_algorithms_8hpp}\index{include/Algorithms.hpp@{include/Algorithms.hpp}}


Defines common algorithms like Multi-\/\+Layer Perceptron (MLP).  


{\ttfamily \#include $<$iostream$>$}\newline
{\ttfamily \#include $<$algorithm$>$}\newline
{\ttfamily \#include "{}Layer.\+hpp"{}}\newline
{\ttfamily \#include "{}Type\+Defs.\+hpp"{}}\newline
Include dependency graph for Algorithms.\+hpp\+:
% FIG 0
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classmicrogradpp_1_1algorithms_1_1_m_l_p}{microgradpp\+::algorithms\+::\+MLP}}
\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Defines common algorithms like Multi-\/\+Layer Perceptron (MLP). 

This file is part of the microgradpp project, a lightweight C++ library for neural network training and inference.\hypertarget{_value_8hpp_License}{}\doxysubsection{\texorpdfstring{License}{License}}\label{_value_8hpp_License}
This program is free software\+: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with this program. If not, see \href{https://www.gnu.org/licenses/}{\texttt{ https\+://www.\+gnu.\+org/licenses/}}.\hypertarget{_value_8hpp_Author}{}\doxysubsection{\texorpdfstring{Author}{Author}}\label{_value_8hpp_Author}
Gautam Sharma Email\+: \href{mailto:gautamsharma2813@gmail.com}{\texttt{ gautamsharma2813@gmail.\+com}} Date\+: September 29, 2024

This header file contains the definition of the MLP (Multi-\/\+Layer Perceptron) class, which implements a feedforward neural network with support for various layer sizes and activation functions. The class provides methods for forward propagation, updating parameters, and zeroing gradients. 