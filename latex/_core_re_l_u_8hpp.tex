\doxysection{include/core/\+Core\+Re\+LU.hpp File Reference}
\hypertarget{_core_re_l_u_8hpp}{}\label{_core_re_l_u_8hpp}\index{include/core/CoreReLU.hpp@{include/core/CoreReLU.hpp}}


Defines the Core\+Re\+LU class for applying Re\+LU activation in neural networks.  


{\ttfamily \#include $<$iostream$>$}\newline
{\ttfamily \#include $<$cassert$>$}\newline
{\ttfamily \#include "{}Mpp\+Core.\+hpp"{}}\newline
{\ttfamily \#include "{}Activation.\+hpp"{}}\newline
Include dependency graph for Core\+Re\+LU.\+hpp\+:
% FIG 0
This graph shows which files directly or indirectly include this file\+:
% FIG 1
\doxysubsubsection*{Classes}
\begin{DoxyCompactItemize}
\item 
class \mbox{\hyperlink{classmicrogradpp_1_1core_1_1_core_re_l_u}{microgradpp\+::core\+::\+Core\+Re\+LU}}
\begin{DoxyCompactList}\small\item\em Implements the Re\+LU activation function as a layer in a neural network. \end{DoxyCompactList}\end{DoxyCompactItemize}


\doxysubsection{Detailed Description}
Defines the Core\+Re\+LU class for applying Re\+LU activation in neural networks. 

This file is part of the microgradpp project, a lightweight C++ library for neural network training and inference.\hypertarget{_value_8hpp_License}{}\doxysubsection{\texorpdfstring{License}{License}}\label{_value_8hpp_License}
This program is free software\+: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with this program. If not, see \href{https://www.gnu.org/licenses/}{\texttt{ https\+://www.\+gnu.\+org/licenses/}}.\hypertarget{_value_8hpp_Author}{}\doxysubsection{\texorpdfstring{Author}{Author}}\label{_value_8hpp_Author}
Gautam Sharma Email\+: \href{mailto:gautamsharma2813@gmail.com}{\texttt{ gautamsharma2813@gmail.\+com}} Date\+: October 14, 2024

The {\ttfamily Core\+Re\+LU} class provides a Re\+LU (Rectified Linear Unit) activation layer, applying the Re\+LU function element-\/wise to an input tensor. This class inherits from {\ttfamily Mpp\+Core} and overrides the necessary methods for a basic Re\+LU layer in neural networks. 