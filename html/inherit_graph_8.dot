digraph "Graphical Class Hierarchy"
{
 // LATEX_PDF_SIZE
  bgcolor="transparent";
  edge [fontname=Helvetica,fontsize=10,labelfontname=Helvetica,labelfontsize=10];
  node [fontname=Helvetica,fontsize=10,shape=box,height=0.2,width=0.4];
  rankdir="LR";
  Node0 [id="Node000000",label="microgradpp::core::\lMppCore",height=0.2,width=0.4,color="grey40", fillcolor="white", style="filled",URL="$classmicrogradpp_1_1core_1_1_mpp_core.html",tooltip="Abstract base class for core components of neural network layers."];
  Node0 -> Node1 [id="edge6_Node000000_Node000001",dir="back",color="steelblue1",style="solid",tooltip=" "];
  Node1 [id="Node000001",label="microgradpp::core::\lCoreLinear",height=0.2,width=0.4,color="grey40", fillcolor="white", style="filled",URL="$classmicrogradpp_1_1core_1_1_core_linear.html",tooltip="Represents a linear (fully connected) layer with configurable input and output dimensions."];
  Node0 -> Node2 [id="edge7_Node000000_Node000002",dir="back",color="steelblue1",style="solid",tooltip=" "];
  Node2 [id="Node000002",label="microgradpp::core::\lCoreReLU",height=0.2,width=0.4,color="grey40", fillcolor="white", style="filled",URL="$classmicrogradpp_1_1core_1_1_core_re_l_u.html",tooltip="Implements the ReLU activation function as a layer in a neural network."];
}
